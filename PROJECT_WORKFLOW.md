# ğŸŒŠ Federated Learning for Flood Damage Detection - Complete Workflow

## ğŸ“‹ Table of Contents
1. [Project Overview](#project-overview)
2. [System Architecture](#system-architecture)
3. [SimpleCNN Model - Deep Dive](#simplecnn-model---deep-dive)
4. [Data Flow & Connections](#data-flow--connections)
5. [Component Details](#component-details)
6. [Workflow Execution](#workflow-execution)
7. [Communication Protocol](#communication-protocol)
8. [Privacy & Security](#privacy--security)

---

## ğŸ¯ Project Overview

### Purpose
This project implements a **Federated Learning (FL)** system for flood damage detection using the Flower framework and PyTorch. It demonstrates privacy-preserving collaborative AI where multiple clients train a shared model without sharing their raw data.

### Key Features
- âœ… **Privacy-Preserving**: Raw images never leave client devices
- âœ… **Collaborative Learning**: Multiple clients improve a shared model
- âœ… **Real-time Monitoring**: Streamlit dashboard for live tracking
- âœ… **Binary Classification**: Flooded vs Not Flooded images
- âœ… **CPU-Only**: Designed for lightweight deployment

### Technology Stack
- **Framework**: Flower (flwr) - Federated Learning
- **Deep Learning**: PyTorch & torchvision
- **Visualization**: Matplotlib, Streamlit
- **Language**: Python 3.x

---

## ğŸ—ï¸ System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEDERATED LEARNING SYSTEM                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SERVER (server.py) â”‚              â”‚ STREAMLIT DASHBOARD  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚              â”‚   (streamlit_app.py) â”‚
â”‚  â”‚  Global Model  â”‚  â”‚              â”‚                      â”‚
â”‚  â”‚   SimpleCNN    â”‚  â”‚              â”‚  ğŸ“ˆ Live Accuracy    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚              â”‚  ğŸ“Š Round Progress   â”‚
â”‚         â”‚            â”‚              â”‚  ğŸ”’ Privacy Stats    â”‚
â”‚    FedAvg Strategy   â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚    127.0.0.1:8080    â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
         â”‚                                      â”‚
         â”‚ gRPC Connection                 metrics.json
         â”‚                                      â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚          â”‚              â”‚
    â–¼          â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Client 1â”‚ â”‚Client 2â”‚ â”‚Client 3â”‚
â”‚  (cid=1)â”‚ â”‚ (cid=2)â”‚ â”‚ (cid=3)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Local  â”‚ â”‚ Local  â”‚ â”‚ Local  â”‚
â”‚ Data   â”‚ â”‚ Data   â”‚ â”‚ Data   â”‚
â”‚ Model  â”‚ â”‚ Model  â”‚ â”‚ Model  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        GLOBAL TEST SET
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  data/global_testâ”‚
    â”‚   - flooded/     â”‚
    â”‚   - not_flooded/ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Directory Structure

```
flwr-flood-damage/
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ client_1/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”‚   â”œâ”€â”€ flooded/       # Training images for client 1
â”‚   â”‚   â”‚   â””â”€â”€ not_flooded/
â”‚   â”‚   â””â”€â”€ test/
â”‚   â”‚       â”œâ”€â”€ flooded/       # Local test images for client 1
â”‚   â”‚       â””â”€â”€ not_flooded/
â”‚   â”œâ”€â”€ client_2/              # Same structure as client_1
â”‚   â”œâ”€â”€ client_3/              # Same structure as client_1
â”‚   â””â”€â”€ global_test/
â”‚       â”œâ”€â”€ flooded/           # Held-out global test set
â”‚       â””â”€â”€ not_flooded/
â”‚
â”œâ”€â”€ ğŸ Core Files:
â”‚   â”œâ”€â”€ models.py              # SimpleCNN architecture
â”‚   â”œâ”€â”€ client.py              # Flower client implementation
â”‚   â”œâ”€â”€ server.py              # Flower server with FedAvg
â”‚   â”œâ”€â”€ dataset_loader.py      # Data loading utilities
â”‚   â”œâ”€â”€ utils.py               # Helper functions
â”‚   â”‚
â”‚   â”œâ”€â”€ simple_demo.py         # Simplified FL demo (recommended)
â”‚   â””â”€â”€ streamlit_app.py       # Live dashboard
â”‚
â”œâ”€â”€ ğŸ“Š Output Files:
â”‚   â”œâ”€â”€ metrics.json           # Training metrics for dashboard
â”‚   â””â”€â”€ accuracy_curve.png     # Final accuracy plot
â”‚
â””â”€â”€ ğŸ“„ Configuration:
    â””â”€â”€ requirements.txt       # Python dependencies
```

---

## ğŸ§  SimpleCNN Model - Deep Dive

### Architecture Overview

The `SimpleCNN` is a lightweight Convolutional Neural Network designed for **64x64 RGB images** with **binary classification** (flooded vs not_flooded).

### Complete Architecture

```python
SimpleCNN(
  num_classes=2,      # Binary classification
  in_channels=3       # RGB images
)
```

### Layer-by-Layer Breakdown

#### **Input Layer**
- **Input Shape**: `[batch_size, 3, 64, 64]`
- **3 Channels**: RGB color images
- **64x64**: Fixed image resolution

---

#### **Layer 1: First Convolutional Block**

```python
self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)
```

**Parameters:**
- **Input Channels**: 3 (RGB)
- **Output Channels**: 16 feature maps
- **Kernel Size**: 3x3
- **Padding**: 1 (maintains spatial dimensions)
- **Stride**: 1 (default)
- **Activation**: ReLU

**Computation:**
- Input: `[batch, 3, 64, 64]`
- After Conv2d: `[batch, 16, 64, 64]`
- After ReLU: `[batch, 16, 64, 64]` (non-linearity)
- After MaxPool2d(2,2): `[batch, 16, 32, 32]` (downsampling by 2)

**Learnable Parameters:**
- Weights: `3 Ã— 16 Ã— 3 Ã— 3 = 432`
- Biases: `16`
- **Total**: 448 parameters

**What it learns**: Low-level features like edges, corners, textures, color gradients

---

#### **Layer 2: Second Convolutional Block**

```python
self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
```

**Parameters:**
- **Input Channels**: 16 (from conv1)
- **Output Channels**: 32 feature maps
- **Kernel Size**: 3x3
- **Padding**: 1
- **Stride**: 1

**Computation:**
- Input: `[batch, 16, 32, 32]`
- After Conv2d: `[batch, 32, 32, 32]`
- After ReLU: `[batch, 32, 32, 32]`
- After MaxPool2d(2,2): `[batch, 32, 16, 16]` (downsampling by 2)

**Learnable Parameters:**
- Weights: `16 Ã— 32 Ã— 3 Ã— 3 = 4,608`
- Biases: `32`
- **Total**: 4,640 parameters

**What it learns**: Mid-level features like water patterns, object shapes, flood characteristics

---

#### **Layer 3: Flattening**

```python
x = x.view(x.size(0), -1)
```

**Operation:**
- Reshapes feature maps into 1D vector
- Input: `[batch, 32, 16, 16]`
- Output: `[batch, 32*16*16]` = `[batch, 8192]`

**No learnable parameters**

---

#### **Layer 4: First Fully Connected Layer**

```python
self.fc1 = nn.Linear(in_features=32*16*16, out_features=128)
```

**Parameters:**
- **Input Features**: 8,192 (flattened feature maps)
- **Output Features**: 128 neurons
- **Activation**: ReLU
- **Dropout**: 0.25 (25% of neurons randomly dropped during training)

**Computation:**
- Input: `[batch, 8192]`
- After Linear: `[batch, 128]`
- After ReLU: `[batch, 128]`
- After Dropout(0.25): `[batch, 128]` (random masking)

**Learnable Parameters:**
- Weights: `8,192 Ã— 128 = 1,048,576`
- Biases: `128`
- **Total**: 1,048,704 parameters

**What it learns**: High-level abstract features, decision boundaries

---

#### **Layer 5: Output Layer**

```python
self.fc2 = nn.Linear(in_features=128, out_features=num_classes)
```

**Parameters:**
- **Input Features**: 128
- **Output Features**: 2 (flooded, not_flooded)
- **No Activation** (logits output)

**Computation:**
- Input: `[batch, 128]`
- Output: `[batch, 2]` (raw scores/logits)

**Learnable Parameters:**
- Weights: `128 Ã— 2 = 256`
- Biases: `2`
- **Total**: 258 parameters

---

### Complete Forward Pass Example

```python
# Input: batch of 32 images (64x64 RGB)
Input:      [32, 3, 64, 64]

# Convolutional Block 1
Conv1:      [32, 16, 64, 64]   # 16 feature maps
ReLU:       [32, 16, 64, 64]
MaxPool:    [32, 16, 32, 32]   # Reduce spatial size by 2

# Convolutional Block 2
Conv2:      [32, 32, 32, 32]   # 32 feature maps
ReLU:       [32, 32, 32, 32]
MaxPool:    [32, 32, 16, 16]   # Reduce spatial size by 2

# Fully Connected Layers
Flatten:    [32, 8192]         # 32*16*16 = 8192
FC1:        [32, 128]
ReLU:       [32, 128]
Dropout:    [32, 128]          # 25% dropout during training
FC2:        [32, 2]            # Output logits

# Loss Computation (CrossEntropyLoss)
# Applies softmax internally and computes negative log-likelihood
Output:     [32, 2]            # Class probabilities after softmax
```

---

### Model Statistics

#### **Total Parameters**
```
Layer         | Parameters
--------------|------------
conv1         | 448
conv2         | 4,640
fc1           | 1,048,704
fc2           | 258
--------------|------------
TOTAL         | 1,054,050
```

#### **Memory Footprint**
- **Parameters**: 1,054,050 Ã— 4 bytes (float32) â‰ˆ **4.01 MB**
- **Activations** (single image forward pass): â‰ˆ **0.5 MB**
- **Total Model Size**: â‰ˆ **4.5 MB**

#### **Computational Complexity**
- **FLOPs** (single image): â‰ˆ **10.7 M** (million operations)
- **Inference Time** (CPU): â‰ˆ **5-10 ms** per image

---

### Loss Function & Training

#### **CrossEntropyLoss**
```python
criterion = nn.CrossEntropyLoss()
```

**How it works:**
1. Takes raw logits from model output `[batch, 2]`
2. Applies softmax to convert to probabilities
3. Computes negative log-likelihood loss

**Mathematical Formula:**
```
Loss = -log(P(correct_class))

where P(class) = exp(logit_class) / sum(exp(logits))
```

**Example:**
```python
# Model output (logits)
logits = [2.5, -0.3]  # [flooded_score, not_flooded_score]

# Softmax
probabilities = [0.942, 0.058]  # High confidence: flooded

# If true label is "flooded"
loss = -log(0.942) = 0.059  # Low loss (good prediction)

# If true label is "not_flooded"
loss = -log(0.058) = 2.85   # High loss (bad prediction)
```

---

### Optimizer

#### **Adam Optimizer**
```python
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
```

**Parameters:**
- **Learning Rate**: 0.001 (1e-3)
- **Betas**: (0.9, 0.999) - default momentum parameters
- **Epsilon**: 1e-8 - numerical stability
- **Weight Decay**: 0 - no L2 regularization

**Why Adam?**
- Adaptive learning rates per parameter
- Combines momentum and RMSprop
- Works well with sparse gradients
- Less sensitive to hyperparameters

---

### Regularization Techniques

#### **1. Dropout (0.25)**
- Applied after first FC layer
- Randomly sets 25% of activations to 0 during training
- Prevents overfitting by forcing redundancy
- **Only active during training**, disabled during evaluation

#### **2. MaxPooling**
- Reduces spatial dimensions by 2x
- Provides translation invariance
- Reduces overfitting by limiting parameters

#### **3. Small Architecture**
- Only 1M parameters (vs ResNet50: 25M)
- Reduces overfitting on small datasets
- Faster training and inference

---

### Why This Architecture Works

#### **Design Choices**

1. **Small Input Size (64x64)**
   - Faster training
   - Lower memory usage
   - Still captures flood characteristics
   - Suitable for CPU training

2. **Two Convolutional Layers**
   - First layer: Low-level features (edges, textures)
   - Second layer: Mid-level features (water patterns, objects)
   - Sufficient for binary classification

3. **Gradual Channel Expansion (3â†’16â†’32)**
   - Captures increasing complexity
   - Balances expressiveness and efficiency

4. **Single FC Layer (128 neurons)**
   - Compact representation
   - Prevents overfitting
   - Fast inference

5. **No Batch Normalization**
   - Simpler architecture
   - Avoids complications in federated learning
   - Works well for small models

---

### Training Characteristics

#### **Single Epoch Training**
```python
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()  # Enable dropout and gradient tracking
    for images, labels in loader:
        images, labels = images.to(device), labels.to(device)
        
        # Forward pass
        outputs = model(images)           # [batch, 2]
        loss = criterion(outputs, labels)  # Scalar
        
        # Backward pass
        optimizer.zero_grad()              # Clear old gradients
        loss.backward()                    # Compute gradients
        optimizer.step()                   # Update weights
```

**What happens:**
1. Model processes one batch at a time
2. Computes predictions
3. Calculates loss compared to true labels
4. Backpropagates error through network
5. Updates all 1M+ parameters using Adam

#### **Evaluation**
```python
def evaluate(model, loader, device, criterion=None):
    model.eval()  # Disable dropout
    with torch.no_grad():  # No gradient computation
        for images, labels in loader:
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)  # Get class with highest score
            correct += (predicted == labels).sum()
```

---

## ğŸ”„ Data Flow & Connections

### Overall Communication Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEDERATED ROUND CYCLE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. INITIALIZATION
   Server                          Clients
     â”‚                               â”‚
     â”‚  Initial Parameters (1M)      â”‚
     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚
     â”‚                               â”‚

2. LOCAL TRAINING (Parallel)
   Server                    Client 1    Client 2    Client 3
     â”‚                          â”‚           â”‚           â”‚
     â”‚                       [Train]     [Train]     [Train]
     â”‚                       on Local   on Local   on Local
     â”‚                         Data       Data       Data
     â”‚                          â”‚           â”‚           â”‚

3. MODEL AGGREGATION
   Server                    Client 1    Client 2    Client 3
     â”‚                          â”‚           â”‚           â”‚
     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚           â”‚           â”‚
     â”‚    Updated Weights       â”‚           â”‚           â”‚
     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚           â”‚
     â”‚    Updated Weights                   â”‚           â”‚
     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
     â”‚    Updated Weights                               â”‚
     â”‚                                                   â”‚
     â”‚  FedAvg Aggregation                              â”‚
     â”‚  (Weighted Average)                              â”‚
     â”‚                                                   â”‚

4. GLOBAL EVALUATION
   Server
     â”‚
     â”‚  Test on Global Dataset
     â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
     â”‚                         Accuracy: 0.85
     â”‚                         Loss: 0.42
     â”‚  metrics.json â”€â”€â–º Streamlit Dashboard

5. NEXT ROUND (Repeat 1-4)
```

---

### Detailed Connection Protocol

#### **Step-by-Step Communication**

**Phase 1: Server Startup**
```
1. Server starts at 127.0.0.1:8080
   - Loads global test dataset
   - Initializes SimpleCNN model
   - Creates initial parameters (1,054,050 values)
   - Sets up FedAvg strategy
   - Waits for client connections
```

**Phase 2: Client Connection**
```
2. Each client connects via gRPC
   Client 1: python client.py --cid 1
   Client 2: python client.py --cid 2
   Client 3: python client.py --cid 3
   
   Each client:
   - Connects to 127.0.0.1:8080
   - Loads local data (train + test)
   - Initializes local SimpleCNN model
   - Registers with server
   - Waits for training instructions
```

**Phase 3: Federated Round Execution**
```
3. Server initiates round (e.g., Round 1)
   
   Server â†’ Client 1, 2, 3:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ fit() RPC call                   â”‚
   â”‚ - parameters: [1M numpy arrays]  â”‚
   â”‚ - config: {                      â”‚
   â”‚     "epochs": 1,                 â”‚
   â”‚     "batch_size": 32             â”‚
   â”‚   }                              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Phase 4: Client Local Training**
```
4. Each client (in parallel):
   
   a) Receives global parameters
      set_parameters_to_model(model, parameters)
   
   b) Local training
      for epoch in range(1):
          for batch in train_loader:
              images, labels = batch
              outputs = model(images)
              loss = criterion(outputs, labels)
              loss.backward()
              optimizer.step()
   
   c) Local evaluation
      test_loss, test_acc = evaluate(model, test_loader)
   
   d) Returns to server:
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Updated parameters: [1M arrays]  â”‚
      â”‚ num_examples: 120 (train samples)â”‚
      â”‚ metrics: {"accuracy": 0.78}      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Phase 5: Server Aggregation**
```
5. Server receives updates from all clients
   
   Client 1: params_1, n_1=120, acc_1=0.78
   Client 2: params_2, n_2=130, acc_2=0.81
   Client 3: params_3, n_3=110, acc_3=0.75
   
   FedAvg Algorithm:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ total_samples = 120 + 130 + 110 = 360 â”‚
   â”‚                                        â”‚
   â”‚ For each parameter i:                  â”‚
   â”‚   weight_1 = 120/360 = 0.333          â”‚
   â”‚   weight_2 = 130/360 = 0.361          â”‚
   â”‚   weight_3 = 110/360 = 0.306          â”‚
   â”‚                                        â”‚
   â”‚   global_param[i] =                    â”‚
   â”‚     0.333 Ã— params_1[i] +             â”‚
   â”‚     0.361 Ã— params_2[i] +             â”‚
   â”‚     0.306 Ã— params_3[i]               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Phase 6: Global Evaluation**
```
6. Server evaluates aggregated model
   
   set_parameters_to_model(global_model, aggregated_params)
   
   Evaluate on global_test/:
   - 100 flooded images
   - 100 not_flooded images
   
   Results:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Round 1:                 â”‚
   â”‚   Accuracy: 0.845        â”‚
   â”‚   Loss: 0.428            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   Save to metrics.json:
   {
     "accuracies": [0.765, 0.845],  # [initial, round1]
     "last_updated": "2025-11-19T08:30:00",
     "training_complete": false
   }
```

**Phase 7: Next Round**
```
7. Repeat Phase 3-6 for configured rounds
   
   Round 1: Acc 0.845
   Round 2: Acc 0.872
   Round 3: Acc 0.891
   ...
   Round N: Acc 0.918
   
   Final:
   - training_complete: true
   - Plot accuracy curve
   - Save final model
```

---

### Network Protocol Details

#### **gRPC Communication**

**Protocol**: gRPC (Google Remote Procedure Call)
**Transport**: HTTP/2
**Serialization**: Protocol Buffers (protobuf)
**Connection**: Bidirectional streaming

**Message Types:**

1. **Parameters Message**
   ```protobuf
   message Parameters {
     repeated bytes tensors = 1;  // Serialized numpy arrays
     string tensor_type = 2;      // "numpy.ndarray"
   }
   ```

2. **FitIns (Server â†’ Client)**
   ```protobuf
   message FitIns {
     Parameters parameters = 1;    // Global model weights
     map<string, Scalar> config = 2;  // Training configuration
   }
   ```

3. **FitRes (Client â†’ Server)**
   ```protobuf
   message FitRes {
     Parameters parameters = 1;    // Updated model weights
     int64 num_examples = 2;       // Number of training samples
     map<string, Scalar> metrics = 3;  // Client metrics
   }
   ```

---

### Data Serialization

**Parameter Transmission:**
```python
# Client side: Model â†’ numpy â†’ bytes
parameters = []
for name, param in model.state_dict().items():
    numpy_array = param.detach().cpu().numpy()  # Tensor â†’ numpy
    parameters.append(numpy_array)               # Add to list

# Flower serializes to bytes for transmission
# Size: ~4 MB (1M params Ã— 4 bytes/float32)

# Server side: bytes â†’ numpy â†’ Model
state_dict = {}
for (name, _), param_array in zip(model.state_dict().items(), parameters):
    state_dict[name] = torch.tensor(param_array)
model.load_state_dict(state_dict)
```

---

## ğŸ“¦ Component Details

### 1. models.py - SimpleCNN Architecture

**Purpose**: Defines the neural network architecture

**Key Features:**
- Lightweight CNN for 64x64 images
- 2 convolutional layers (16â†’32 channels)
- 2 fully connected layers (8192â†’128â†’num_classes)
- Dropout (0.25) for regularization
- 1,054,050 total parameters

**Usage:**
```python
model = SimpleCNN(num_classes=2)  # Binary classification
```

---

### 2. dataset_loader.py - Data Management

**Purpose**: Load and preprocess image datasets

**Key Functions:**

**a) `build_transforms()`**
```python
# Preprocessing pipeline
transforms.Compose([
    transforms.Resize((64, 64)),           # Resize to 64x64
    transforms.ToTensor(),                 # Convert to tensor [0,1]
    transforms.Normalize(                  # Normalize to [-1,1]
        mean=[0.5, 0.5, 0.5],
        std=[0.5, 0.5, 0.5]
    )
])
```

**b) `load_imagefolder_dataloaders()`**
- Loads train/test data for a client
- Uses PyTorch ImageFolder (class-based directory structure)
- Returns DataLoaders and num_classes

**c) `train_one_epoch()`**
- Trains model for one full pass through training data
- Updates model weights via backpropagation

**d) `evaluate()`**
- Tests model on validation/test data
- Returns loss and accuracy

---

### 3. client.py - Federated Client

**Purpose**: Implements Flower client for federated learning

**Class: FlowerClient**

**Initialization:**
```python
def __init__(self, cid, batch_size=32, lr=1e-3):
    # Load client-specific data
    self.train_loader, self.test_loader, num_classes = 
        get_loaders_for_client(cid, batch_size)
    
    # Initialize model
    self.model = SimpleCNN(num_classes=num_classes)
    self.criterion = nn.CrossEntropyLoss()
    self.optimizer = optim.Adam(self.model.parameters(), lr=lr)
```

**Key Methods:**

**a) `get_parameters(config)`**
- Extracts current model weights as numpy arrays
- Called by server to fetch client's model

**b) `fit(parameters, config)`**
- Receives global parameters from server
- Trains model locally for N epochs
- Evaluates on local test set
- Returns updated parameters + metrics

**c) `evaluate(parameters, config)`**
- Evaluates given parameters on local test set
- Returns loss, num_examples, and metrics

**Communication Flow:**
```
Server calls fit()
    â†“
Client loads global parameters
    â†“
Client trains on local data
    â†“
Client evaluates locally
    â†“
Client returns updated parameters
    â†“
Server aggregates all clients
```

---

### 4. server.py - Federated Server

**Purpose**: Orchestrates federated learning rounds

**Key Components:**

**a) Global Model Initialization**
```python
# Create model for server-side evaluation
model = SimpleCNN(num_classes=num_classes).to(device)
initial_parameters = get_parameters_from_model(model)
```

**b) FedAvg Strategy**
```python
strategy = fl.server.strategy.FedAvg(
    evaluate_fn=get_evaluate_fn(...),        # Server-side evaluation
    on_fit_config_fn=get_on_fit_config_fn(...),  # Training config
    initial_parameters=initial_parameters     # Starting weights
)
```

**c) Evaluation Function**
```python
def evaluate(server_round, parameters, config):
    # Update global model with aggregated parameters
    set_parameters_to_model(model, parameters)
    
    # Evaluate on global test set
    loss, acc = evaluate(model, global_test_loader, device)
    
    # Save metrics for dashboard
    round_accuracies.append(acc)
    save_metrics(round_accuracies)
    
    return loss, {"accuracy": acc}
```

**d) Server Loop**
```
For each round (1 to num_rounds):
    1. Send global model to clients
    2. Clients train locally
    3. Receive updated models from clients
    4. Aggregate models (FedAvg)
    5. Evaluate aggregated model on global_test
    6. Save metrics
    7. Repeat
```

---

### 5. utils.py - Helper Functions

**Purpose**: Shared utility functions

**a) `get_device()`**
- Returns CPU device (no GPU needed)

**b) `get_parameters_from_model(model)`**
- Extracts model parameters as list of numpy arrays
- Maintains consistent ordering via state_dict()

**c) `set_parameters_to_model(model, parameters)`**
- Loads numpy arrays back into model
- Critical for federated parameter exchange

**Example:**
```python
# Extract parameters
params = get_parameters_from_model(model)
# params = [conv1.weight, conv1.bias, conv2.weight, ..., fc2.bias]
# Length: 10 (5 layers Ã— 2 params each)

# Load parameters
set_parameters_to_model(model, params)
```

---

### 6. simple_demo.py - Simplified Federated Learning

**Purpose**: Standalone demo without client-server complexity

**Workflow:**
```python
1. Load data for all 3 clients
2. Initialize global model + 3 client models
3. Evaluate initial global accuracy

For each round:
    4. Send global parameters to clients
    5. Each client trains locally
    6. Collect client updates
    7. Aggregate via FedAvg (weighted average)
    8. Update global model
    9. Evaluate on global test set
    10. Save metrics.json
    11. Pause 5 seconds (for dashboard demo)

12. Plot final accuracy curve
13. Display privacy statistics
```

**Advantages:**
- No networking issues
- Simpler to understand
- Perfect for presentations/demos
- Shows federated learning concepts clearly

---

### 7. streamlit_app.py - Live Dashboard

**Purpose**: Real-time visualization of training progress

**Features:**
- ğŸ“ˆ **Line chart**: Accuracy vs rounds
- ğŸ¯ **Key metrics**: Current accuracy, best accuracy, improvement
- ğŸ”’ **Privacy stats**: Model size, number of clients, data sharing
- ğŸ”„ **Auto-refresh**: Updates every N seconds
- ğŸ’¾ **Export**: Download metrics as JSON

**Data Source:**
```python
# Reads metrics.json every refresh_rate seconds
{
    "accuracies": [0.76, 0.84, 0.87, 0.89],
    "last_updated": "2025-11-19T08:30:00",
    "training_complete": false,
    "rounds_expected": 5
}
```

**Display Logic:**
```
No data yet â†’ Show "Waiting for FL to start"
Training in progress â†’ Live updates with auto-refresh
Training complete â†’ Static display with final results
```

---

## âš™ï¸ Workflow Execution

### Method 1: Full Federated Learning (Client-Server)

**Terminal 1: Start Server**
```bash
# Activate virtual environment
.venv\Scripts\activate

# Start server (waits for 3 clients by default)
python server.py --num_rounds 5 --epochs 1 --batch_size 32

# Output:
# [Server] Waiting for 2 clients to connect...
# [Server] Starting federated learning
```

**Terminal 2-4: Start Clients**
```bash
# Terminal 2
python client.py --cid 1 --batch_size 32 --lr 0.001

# Terminal 3
python client.py --cid 2 --batch_size 32 --lr 0.001

# Terminal 4
python client.py --cid 3 --batch_size 32 --lr 0.001

# Each client output:
# [Client 1] Loading data from data/client_1/
# [Client 1] Connected to server 127.0.0.1:8080
# [Client 1] Round 1: Training...
# [Client 1] Local eval -> loss: 0.4523, acc: 0.7800
```

**Terminal 5: Dashboard (Optional)**
```bash
streamlit run streamlit_app.py

# Opens browser at http://localhost:8501
# Live updates every 2 seconds
```

---

### Method 2: Simplified Demo (Recommended)

**Terminal 1: Run Demo**
```bash
python simple_demo.py

# Output:
# ğŸŒŠ FEDERATED LEARNING FOR FLOOD DAMAGE DETECTION
# ========================================================
# Demonstrating privacy-preserving collaborative AI
# âœ… Each client keeps data private
# âœ… Only model weights are shared
# âœ… Collective intelligence without data sharing
# 
# ğŸ“ LOADING CLIENT DATA:
#    Client 1: 120 train, 30 test images
#    Client 2: 130 train, 32 test images
#    Client 3: 110 train, 28 test images
#    Global test: 200 images
# 
# ğŸ¤– INITIALIZING MODELS:
#    SimpleCNN architecture: 1,054,050 parameters
#    3 client models initialized
# 
# ğŸ¯ INITIAL GLOBAL EVALUATION:
#    Initial global accuracy: 0.7650
# 
# ğŸ”„ FEDERATED ROUND 1
# ==================================================
# 
# ğŸ“± CLIENT 1 LOCAL TRAINING:
#    Local train loss: 0.4523
#    Local test loss: 0.4321, acc: 0.7800
# 
# ğŸ“± CLIENT 2 LOCAL TRAINING:
#    Local train loss: 0.4234
#    Local test loss: 0.4012, acc: 0.8125
# 
# ğŸ“± CLIENT 3 LOCAL TRAINING:
#    Local train loss: 0.4789
#    Local test loss: 0.4654, acc: 0.7500
# 
# ğŸŒ SERVER AGGREGATION:
# ğŸ¯ GLOBAL TEST ACCURACY AFTER ROUND 1: 0.8450
#    Improvement: +0.0800
#    ğŸ“Š Streamlit metrics updated: 2 points
#    â¸ï¸  Pause for live dashboard update (5 seconds)...
#    â±ï¸  Continuing in 5s...4s...3s...2s...1s...
#    âœ… Continuing to next round...
# 
# [... Rounds 2-3 ...]
# 
# ğŸ† FINAL RESULTS:
# ========================================
# Initial:   0.7650
# Round 1:   0.8450 (+0.0800)
# Round 2:   0.8750 (+0.1100)
# Round 3:   0.8950 (+0.1300)
# 
# Total improvement: 0.1300
# Privacy preserved: âœ… No raw images shared
# Collaboration achieved: âœ… Better accuracy through federation
# 
# ğŸ”’ PRIVACY STATISTICS:
#    Model weights shared: ~4018.2 KB per round
#    Average client data: ~3000.0 KB
#    Privacy factor: 0.7x smaller transmission
# 
# âœ¨ FEDERATED LEARNING DEMONSTRATION COMPLETE! âœ¨
# 
# ğŸŒ STREAMLIT DASHBOARD: Run `streamlit run streamlit_app.py`
# ğŸ“Š Live accuracy chart available in browser
# ğŸ“ˆ 4 data points ready for visualization
```

**Terminal 2: Dashboard**
```bash
streamlit run streamlit_app.py
# Watch live updates as simple_demo.py runs
```

---

### Execution Timeline

```
Time    | Server/Demo              | Client 1  | Client 2  | Client 3  | Dashboard
--------|--------------------------|-----------|-----------|-----------|------------
0:00    | Initialize               | Connect   | Connect   | Connect   | Launch
0:05    | Send global params       | Receive   | Receive   | Receive   | Waiting
0:10    | Wait for training        | Train     | Train     | Train     | Waiting
0:20    | Receive updates          | Send      | Send      | Send      | Updating
0:25    | Aggregate (FedAvg)       | Idle      | Idle      | Idle      | Updating
0:30    | Evaluate global_test     | Idle      | Idle      | Idle      | Display
0:35    | Save metrics.json        | Idle      | Idle      | Idle      | Refresh
0:40    | Start Round 2            | Receive   | Receive   | Receive   | Display
...     | ...                      | ...       | ...       | ...       | ...
```

---

## ğŸ” Privacy & Security

### What is Shared

**âœ… SHARED:**
- Model parameters (weights & biases) - 1M numbers
- Number of training examples per client
- Evaluation metrics (accuracy, loss)

**âŒ NOT SHARED:**
- Raw images
- Image metadata
- Client identities (beyond client ID)
- Training data labels
- Data distributions

---

### Privacy Guarantees

**1. Data Locality**
```
Client 1 Data                Client 2 Data                Client 3 Data
â”œâ”€â”€ train/                   â”œâ”€â”€ train/                   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ flooded/            â”‚   â”œâ”€â”€ flooded/            â”‚   â”œâ”€â”€ flooded/
â”‚   â””â”€â”€ not_flooded/        â”‚   â””â”€â”€ not_flooded/        â”‚   â””â”€â”€ not_flooded/
â””â”€â”€ test/                    â””â”€â”€ test/                    â””â”€â”€ test/

        â†“                           â†“                           â†“
   Model Updates              Model Updates              Model Updates
   (Parameters Only)          (Parameters Only)          (Parameters Only)
        â†“                           â†“                           â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                              Server Aggregation
                              (FedAvg Algorithm)
```

**Images NEVER leave client devices!**

---

**2. Federated Averaging (FedAvg)**

Mathematical formula:
```
Global Model = Î£ (n_i / N) Ã— Client_i_Model

where:
- n_i = number of samples for client i
- N = total samples across all clients
- Client_i_Model = model weights from client i
```

**Example:**
```
Client 1: 120 samples, weight = 120/360 = 0.333
Client 2: 130 samples, weight = 130/360 = 0.361
Client 3: 110 samples, weight = 110/360 = 0.306

For each parameter (e.g., conv1.weight[0][0][0]):
  global_param = 0.333 Ã— (-0.45) + 0.361 Ã— (-0.52) + 0.306 Ã— (-0.38)
               = -0.454
```

---

**3. Model Inversion Resistance**

**Question**: Can someone reconstruct training images from model weights?

**Answer**: Extremely difficult for this architecture because:
- Model parameters (1M) â‰« Single image (64Ã—64Ã—3 = 12,288 pixels)
- Parameters represent learned patterns, not raw data
- Aggregation from multiple clients adds noise
- Millions of possible image combinations map to same weights

---

**4. Communication Efficiency**

**Per-Round Data Transfer:**
```
Client â†’ Server:
  Model weights: ~4 MB
  Metadata: <1 KB
  Total: ~4 MB

Server â†’ Client:
  Model weights: ~4 MB
  Config: <1 KB
  Total: ~4 MB

Bidirectional per client per round: ~8 MB
```

**Compare to centralized learning:**
```
Centralized approach (sending all images):
  Client 1: 120 images Ã— 25 KB â‰ˆ 3,000 KB = 3 MB
  Client 2: 130 images Ã— 25 KB â‰ˆ 3,250 KB = 3.25 MB
  Client 3: 110 images Ã— 25 KB â‰ˆ 2,750 KB = 2.75 MB
  Total: 9 MB (one-time)

Federated approach:
  Per round: 8 MB Ã— 3 clients = 24 MB
  Over 5 rounds: 120 MB
```

**Note**: Privacy benefit outweighs bandwidth cost for sensitive data!

---

### Security Considerations

**1. No Encryption (Current Implementation)**
- gRPC connection is NOT encrypted
- Suitable for local/trusted networks only
- Production systems should use TLS/SSL

**2. No Authentication**
- Any client can connect with any ID
- No verification of client identity
- Production systems need OAuth/API keys

**3. No Differential Privacy**
- Model updates can leak information
- Advanced attacks (gradient inversion) possible
- Add noise to gradients for stronger privacy

**4. No Secure Aggregation**
- Server sees individual client updates
- Compromised server could analyze updates
- Use secure multi-party computation for production

---

### Production Enhancements

**For Real-World Deployment:**

1. **Add Encryption**
   ```python
   # Use Flower's built-in SSL support
   fl.client.start_client(
       server_address="secure.example.com:8080",
       certificates_path="./certificates"
   )
   ```

2. **Implement Differential Privacy**
   ```python
   # Add noise to gradients
   from opacus import PrivacyEngine
   privacy_engine = PrivacyEngine(model, noise_multiplier=1.0)
   ```

3. **Client Authentication**
   ```python
   # Use API keys or OAuth tokens
   metadata = [("authorization", f"Bearer {api_key}")]
   ```

4. **Secure Aggregation**
   - Implement cryptographic protocols
   - Server never sees individual updates
   - Only sees aggregated result

---

## ğŸ“Š Expected Results

### Typical Accuracy Progression

```
Round  | Global Test Accuracy | Improvement
-------|---------------------|-------------
Init   | 0.50-0.55          | (Random)
1      | 0.65-0.75          | +0.15-0.20
2      | 0.75-0.85          | +0.10-0.10
3      | 0.82-0.90          | +0.07-0.05
4      | 0.87-0.93          | +0.05-0.03
5      | 0.90-0.95          | +0.03-0.02
```

**Factors Affecting Performance:**
- Data quality and diversity
- Data distribution across clients
- Number of training samples per client
- Image complexity
- Number of federated rounds
- Learning rate

---

### Performance Metrics

**Training Time (CPU):**
- Single client epoch: 30-60 seconds (100-200 images)
- Full federated round: 1-2 minutes (3 clients)
- Complete training (5 rounds): 5-10 minutes

**Inference Time:**
- Single image: 5-10 ms (CPU)
- Batch (32 images): 50-100 ms

**Model Size:**
- Parameters: 1,054,050
- Disk size: ~4 MB (float32)
- Memory usage: ~10 MB (with activations)

---

## ğŸ› ï¸ Troubleshooting

### Common Issues

**1. "Data directory not found"**
```bash
# Solution: Ensure data is organized correctly
data/
â”œâ”€â”€ client_1/train/flooded/*.jpg
â”œâ”€â”€ client_1/train/not_flooded/*.jpg
â”œâ”€â”€ client_1/test/flooded/*.jpg
â””â”€â”€ ... (same for client_2, client_3, global_test)
```

**2. "Server timeout waiting for clients"**
```bash
# Solution: Start all clients within 60 seconds of server startup
# Or adjust timeout in server.py
```

**3. "CUDA out of memory"**
```python
# Solution: Code is CPU-only, but if you modified it:
device = torch.device("cpu")  # Force CPU
```

**4. "Port already in use (8080)"**
```bash
# Solution: Kill existing process or change port
python server.py --address 127.0.0.1:8090
python client.py --cid 1 --server_address 127.0.0.1:8090
```

**5. "metrics.json not updating in Streamlit"**
```bash
# Solution: Ensure simple_demo.py or server.py is running
# Check file permissions
# Verify metrics.json exists in project root
```

---

## ğŸ“š Key Concepts Summary

### Federated Learning
- **Decentralized training**: Data stays on client devices
- **Privacy-preserving**: Only model updates shared
- **Collaborative**: Multiple parties improve shared model
- **Communication-efficient**: Periodic synchronization

### FedAvg Algorithm
- Weighted average of client models
- Weights proportional to dataset size
- Balances contribution from all clients
- Simple yet effective aggregation

### SimpleCNN Architecture
- Lightweight for CPU training
- 2 convolutional layers for feature extraction
- 2 fully connected layers for classification
- 1M parameters, ~4 MB size
- Suitable for 64x64 RGB images

### Training Process
- Clients train locally (1 epoch per round)
- Server aggregates after each round
- Global evaluation on held-out test set
- Iterative improvement over rounds

---

## ğŸ“ Learning Outcomes

After understanding this project, you should know:

âœ… How federated learning works end-to-end
âœ… CNN architecture for image classification
âœ… Client-server communication in FL
âœ… FedAvg aggregation algorithm
âœ… Privacy benefits of federated learning
âœ… PyTorch model training and evaluation
âœ… Flower framework basics
âœ… Data preprocessing for image models
âœ… Real-time monitoring with Streamlit

---

## ğŸš€ Next Steps

**For Further Exploration:**

1. **Add More Clients**: Test scalability with 5-10 clients
2. **Experiment with Architecture**: Try deeper CNNs, ResNet, MobileNet
3. **Implement Differential Privacy**: Add Opacus library
4. **Multi-class Classification**: Extend to more flood categories
5. **Real-world Deployment**: Deploy on edge devices (Raspberry Pi, mobile)
6. **Heterogeneous Data**: Simulate non-IID data distributions
7. **Advanced Aggregation**: Try FedProx, FedOpt, FedNova
8. **Model Compression**: Quantization, pruning for efficiency

---

## ğŸ“– References

**Libraries:**
- [Flower Documentation](https://flower.dev/docs/)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [Streamlit Documentation](https://docs.streamlit.io/)

**Papers:**
- McMahan et al. (2017) - "Communication-Efficient Learning of Deep Networks from Decentralized Data" (FedAvg)
- Kairouz et al. (2019) - "Advances and Open Problems in Federated Learning"

**Concepts:**
- Federated Learning
- Convolutional Neural Networks
- Privacy-Preserving Machine Learning
- Distributed Deep Learning

---

**End of Workflow Documentation** ğŸ‰
